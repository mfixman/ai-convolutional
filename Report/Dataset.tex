\subsection{Dataset}

The Cityscapes dataset is a large-scale dataset widely used for training, evaluating and benchmarking algorithms in the fields of computer vision, particularly for models used to perform semantic segmentation and instance segmentation of urban street scenes. It consists of high resolution images captured from a vehicle-mounted camera while driving through 50 different cities in Germany and neighbouring countries. The capturing apparatus is equipped with a dual lens camera for stereoscopic capability, allowing for the possibility of applying stereo vision techniques useful for tasks like depth estimation, 3D reconstruction and scene understanding. 

The scenes in the CityScapes dataset represent a variety of urban settings, seasons, daylight conditions, and weather scenarios, providing robust, real-world environments for training models that need to perform under varied conditions. This dataset has been widely used in research for developing, testing and benchmarking new algorithms for computer vision tasks, gaining a place alongside datasets as iconic as ImageNet, COCO and Pascal VOC. The Cityscapes dataset can be accessed in the following address:
\begin{center}
\url{https://www.cityscapes-dataset.com}
\end{center}

The images that are used for our analysis are located in the folder \textbf{data/leftImg8bit}. These, as the name suggests, are taken from the left lens of the stereo camera system. They form the main high-resolution images (2048$\times$1024 pixels) with an 8-bit color depth that are used for all semantic and instance segmentation tasks using this dataset. Each image is stored in a PNG file format with a size that ranges between 2.0 and 2.5 MB. The images have an RGB color space, which means that there are 3 color channels for each image.

The dataset comes with two different types of image annotations:\\
\textbf{Coarse annotations:} These are coarse level annotations on a large set of 20,000 images (see Figure \ref{fig:cityscapes}(a)). The overlaid colors are used to encode the different semantic classes. In total there are 30 classes such as road, car, traffic light, bicycle etc. These are grouped into 8 broader categories (flat surfaces, humans, vehicles, construction, objects, nature, sky and void). The files are located in \textbf{data/coarse} and are split into a training and a validation set. No testing set is provided with the coarse annotations as this is performed with the fine annotations set. In our analysis we have used exclusively this set of images as they have provided us with a much more substantial and varied number of training examples. 

\textbf{Fine annotations:} These provide detailed, pixel-accurate annotations for 5,000 images (see Figure \ref{fig:cityscapes}(b)). This dataset is split into 2,975 annotations for training, 500 for validation and 1,525 for testing. It is worth mentioning that the ground truth annotations for the testing set are not available directly to the users, who instead need to submit their code to an online evaluation server which provides the quantitative performance metrics. This is done to ensure the fair evaluation and benchmarking of models.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{coarse_example.jpg}
        \caption{Coarse mask}
        \label{fig:sub1}
    \end{subfigure}\hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fine_example.jpg}
        \caption{Fine mask}
        \label{fig:sub2}
    \end{subfigure}
    \caption{Examples of coarse (a) and fine (b) mask annotations superimposed on the corresponding original images}
    \label{fig:cityscapes}
\end{figure}

The cityscapes dataset offers tools that allow for the evaluation and benchmarking of different models. One of the main metrics that is used in the evaluation of per-pixel semantic labelling of images is the Intersection over Union (IoU). This measures the overlap between the predicted segmentation and the ground truth and is expressed as the ratio: $\text{IoU} = \frac{TP}{TP + FP + FN}$ where TP, FP and FN are the numbers of true positive, false positive, and false negative pixels respectively. This formula gives a measure of how well the predicted labels agree with the true labels, across the entire test set. Due to the fact that cityscapes offers two distinct granularities for semantic labelling (classes and categories), they report two separate IoU scores as IoU$_\text{category}$ and IoU$_\text{class}$. Pixels that are labelled as "void" are excluded from these calculations. 

Traditional IoU can be biased towards larger objects (which can dominate the score due to their size), hence another metric called iIoU is designed to provide a more balanced comparison, especially in street scenes where object size variability can be substantial. iIoU is defined as $\text{iIoU} = \frac{iTP}{iTP + FP + iFN}$. Here iTP are true positives that are weighed by the ratio of the class's average instance size to the size of the respective ground truth instance. False negatives are similarly adjusted based on the relative size of the object they belong to. The count of false positives remains unweighed since these do not correspond to a specific instance size (they are areas that are incorrectly labelled as an object). Similarly to IoU, iIou is reported both for categories and classes to provide insight into the model's performance across different levels of semantic detail.