\section{Conclusions and Reflections}

This project was a long and thorough work over the CityScapes dataset.
We changed many metrics, objectives, and models throughout its lifetime, learned from those changes, and changed everything again.

From the models we tried, Swin2-JNet is the clear winner.
Not only it has the highest IoU and \iiouc{} scores of the dataset by a little, as shown in \cref{which_one_wins}, but when testing these models on real-world data in \hyperref[hackneyscapes]{Appendix A} the results are considerably nicer.

Why is this model better than UNet, which has an equivalent size?
We propose several reasons.
\begin{enumerate}[topsep=0pt]
	\item The attention in the Swin2 transformers provide a significant improvement to small details, such as people in the background.
	\item Using transfer learning from a pre-trained model means that a lot of basic data, such as lines of objects, are already learned.
		\begin{enumerate}
			\item Moreover, the use of skip connections with the data from a feature pyramid allows the classifier to learn from several resolutions of this feature.
		\end{enumerate}
	\item Not learning on ``background'' pixels (which were mostly arbitrary) saved some capacity on the model to learn extra details, and helped it generalise more.
		\begin{enumerate}
			\item Note that earlier, smaller experiments with other models resulted in a lot of incorrect positives on these background pixels.
				This property was a last-minute addition, and it might make sense to review the previous experiments.
		\end{enumerate}
	\item Effective use of Dropout prevented bad overfitting.
		It's easy to compare \cref{unet_model_loss} with \cref{swin2_model_loss} and realise that, while early stopping prevented the model from learning many incorrect details, the UNet model is still not able to learn some correct ones due to how badly it overfits.
\end{enumerate}

The most effective part of this project was its infrastructure: having a pre-set files to allow us to train models quickly and easily (along with the help of Wandb) allowed us to iterate quickly and try a lot of experiments.

Additionally, having two canonical metrics we do not train in (IoU and \iiouc{} scores) allowed us to compare different models easily.

Most importantly, the halving parameter sweep in \cref{param_sweep_section} allowed us to find and compare dozens of different hyperparameter combinations in a reasonable amount of time.

The CityScapes dataset did not provide a test set, as it's secret to be submitted\cite{cityscapes_benchmark}.
In the near future we plan to train our Swin2-JNet model classifier in both the training and validation data and submit it to the Benchmark suite.
If our calculations in the validation set are roughly correct, we should be roughly in the best two thirds of \href{https://www.cityscapes-dataset.com/benchmarks/#pixel-level-results}{the CityScapes Benchmark Competition}, despite not even using the fine dataset.

The objective of this project was not to win a competition, which is why it's important to be wary of overfocusing on maximising metrics.
While the IoU score could have been higher, the results in our own subjective test set of \hyperref[hackneyscapes]{HackneyScapes} speak for themselves.
