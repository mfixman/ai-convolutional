\section{Training Methodology}

\subsection{Transforming and Enhancing Data}
In our final runs, the images in the input data are resized to $768 \times 768$.
This size presents a good balance between model performance and training speed: the final results of the models were almost identical to using the entire size of $2048 \times 1024$.
Additionally, this new size makes it possible to use pretrained Swin2 in the entire data instead of having to separate the images of stretch it separately.

The loss of the final mask is calculated on the small, resized image.
When evaluating, the final mask is stretched to its original larger size.

Additionally, we experimented with creating random transforms of the input data.
\begin{itemize}
	\item Random flips.
	\item Random crops.
	\item Random horizontal flips.
\end{itemize}

In different experiments these transforms either replaced or augmented the original training data.
However, this is not present in the final models as their results weren't conclusive: since the original data is complete enough, adding these transforms only made the results on the validation set worse.

\subsection{Optimiser and other Hyperparameters}
There are many possible hyperparameters to enhance the model.
\begin{description}[style=nextline]
	\item[Loss Function]
		Which loss function to use for the model.
		\begin{description}
			\item[Categorical Cross-Entropy] Unweighted towards different classes.
			\item[IoU Loss] A differentiable version of IoU score.
			\item[Dice Loss] Which weights less common classes more.
		\end{description}
		Surprisingly, using categorical cross-entropy provided better results in all metrics, including IoU score.
	\item[Optimiser]
		Which optimiser to use to manage the gradient 
\end{description}
