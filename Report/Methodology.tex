\section{Methodology}

Our code is made up of the following parts: \\
The \textbf{work.py} file sets up the training pipeline for our segmentation task. After loading all the necessary libraries, we use the function \textbf{parse\_args()} to define and parse command-line arguments. Using this function  we specify the model to be used in our experiment, as well as the model-specific parameters like learning rate, weight decay, batch size, number of epochs, optimiser etc. The function can also check for which device to use (cuda, mps or cpu) depending on the platform that we run our code on and handles default and edge cases (like when no image size or batch size is specified). The function very usefully also offers a help option that will list and describe all the available options to the user. 

The \textbf{main()} function orchestrates the execution flow. We initialize random seed for reproducibility, set up logging, and merge the default configuration with the command-line arguments using the parsed results. We initialise \textbf{wandb} for tracking our experiment and prepare the datasets and dataloaders for training and validation. We instantiate the model and move it to the designated device (CPU or GPU) and finally create a \textbf{Trainer} object with the model, dataloaders, and configuration and start the training process by instantiating the \textbf{Trainer()} class and calling the \textbf{train} method. 

The \textbf{Trainer.py} performs several functions. It handles the logging of the best model's weights to wandb as 'Artifacts' for tracking and versioning. It performs a single training step by computing the model's output, calculating the loss using the defined loss criterion and updates the model weights based on the gradients. It also conducts an evaluation step, this time without gradient calculation, and updates the evaluation loss metric \textbf{'eval\_losses'}. The training process is iterated over the number of epochs, during which we execute the training and validation steps, log the performance and check for improvement in validation loss in order to save the best model state. Finally we log the training and validation losses to wandb for monitoring. 

The \textbf{model.py} file enables us to dynamically instantiate different models that are used for our analysis by mapping the model names stored on the \textbf{'models'} dictionary to the corresponding architectures. In the \textbf{CityScapesDataset.py} file we define the \textbf{CityScapesDataset()} class which handles the data loading for the CityScapes dataset. The \textbf{get\_transforms()} function allows us to apply transformations to the images and masks loaded from the dataset. Due to limited computational resources, during most of our experiments, we first reduce the size of the images and masks (usually to 512$\times$512px). For training data, additional random transformations (horizontal flipping and rotation) are applied for data augmentation to improve the robustness of the model. Images are then converted to a tensor with normalised pixel values between [0,1] and masks are converted to a long tensor from a numpy array, which is necessary for the categorical targets in our task. 

The models directory contains the definitions of architectures of the various models we have tried for this analysis. We have started our investigation using a straightforward Fully Convolutional Network (FCN) model in \textbf{SimpleFCN.py}. The FCN is structured with an encoder-decoder design pattern. The encoder consists of three stages, each composed of a convolutional layer followed by a ReLU activation function and a max-pooling layer. This sequence allows us to process the image to extract basic features and reduces its spatial dimensions. In the decoder, we reverse the process of the encoder, using transposed convolutions to progressively reconstruct the spatial dimensions of the output. The final layer of the decoder increases the spatial dimensions back to the size of the original image and adjusts the depth to the number of output classes. 

The second model we have deployed is the U-net model as defined in \textbf{UNetModel.py}. Our U-net model consists of the following components: the \textbf{Block()} class defines a basic convolutional module that is made up of a convolutional layer followed by batch normalisation and a ReLU activation function. The \textbf{Downsampler} module is used to process and downsample the input tensor and is part of our encoder. It consists of two convolutional Block modules followed by a max pooling operation to reduce the spatial dimensions by half. The downsampling step increases the receptive field and allows the network to capture more abstract features at each level. The \textbf{Upsampler} module on the other hand is used as part of the decoder and is responsible for upsampling the feature maps and concatenating them from the corresponding downsampler through skip connections. It uses a transposed convolution to increase the spatial resolution and then processes the concatenated output through two Block modules. Finally, we construct our model by stacking together four Downsampler modules which progressively reduce the spatial dimensions and increase the depth of feature maps (64, 128,256,512 channels). Two Block modules process the deepest features without changing their dimensions. This is the "Bottleneck' part of the network and it bridges the encoding and decoding paths. The decoder is made up of four Upsampler modules which progressively increase the spatial dimensions and reduce the depth of the feature maps. Each Upsampler uses the output from the corresponding Downsampler for concatenation (skip connection), which helps in reconstructing the segmentation maps with detailed features. A final convolutional layer with kernel size 1$\times$1 is used to map the 64 channels back to our number of output channels (classes), therefore providing our segmentation output. 

